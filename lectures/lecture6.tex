\newpage
\section{January 24, 2024}
\begin{example}
    \textbf{Example:} Let $X \sim N(0,1)$. We aim to compute $\varphi(t) = \mathbb{E}[e^{itX}]$ for $t \in \mathbb{R}$.

The characteristic function $\varphi(t)$ is given by:
\[
\varphi(t) = \sum_{k=0}^{\infty} \frac{(it)^k}{k!} \mathbb{E}[X^k] \quad \text{(1)}
\]

We use the property: for differentiable functions $g: \mathbb{R} \to \mathbb{R}$,
\[
\mathbb{E}[g'(X)] = \mathbb{E}[X g(X)] \quad \text{(2)}
\]

Since
\[
\mathbb{E}[g'(X)] = \int_{-\infty}^{\infty} g'(x) \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \, dx = \int_{-\infty}^{\infty} g(x) x \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \, dx = \mathbb{E}[X g(X)],
\]
by integration by parts.

Applying (2) for $g(x) = x^k$, then $g'(x) = k x^{k-1}$. So (2) becomes:
\[
\mathbb{E}[k X^{k-1}] = \mathbb{E}[X \cdot X^{k-1}] \quad \text{(3)}
\]

Hence,
\[
\mathbb{E}[X^k] = k \mathbb{E}[X^{k-1}] \quad \text{for } k \geq 1 \quad \text{(4)}
\]
By symmetry of the standard normal distribution, all odd powers of $X$ have an expected value of zero, i.e., $\mathbb{E}[X^k] = 0$ for $k$ odd.

For even powers, using the property from before:
\begin{align*}
k = 2 & : \quad \mathbb{E}[X^2] = 1, \\
k = 4 & : \quad \mathbb{E}[X^4] = 3 \cdot \mathbb{E}[X^2] = 3, \\
k = 6 & : \quad \mathbb{E}[X^6] = 5 \cdot \mathbb{E}[X^4] = 5 \cdot 3 = 15, \\
\text{and so on.}
\end{align*}
In general, for $k = 2n$:
\[
\mathbb{E}[X^{2n}] = 1 \cdot 3 \cdot 5 \cdots (2n - 1) = (2n-1)!! \quad \text{(double factorial)}
\]

\textbf{Characteristic Function:} Returning to the characteristic function:
\[
\varphi(t) = \sum_{n=0}^{\infty} \frac{(it)^{2n}}{(2n)!} \mathbb{E}[X^{2n}] = \sum_{n=0}^{\infty} \frac{(it)^{2n}}{(2n)!} (2n-1)!! = \sum_{n=0}^{\infty} \frac{(-1)^n t^{2n}}{2^n n!}
\]
where we used the relation $(2n)!/(2n-1)!! = 2^n n!$.

Recalling the Taylor series expansion for $e^{-t^2/2}$, we have:
\[
e^{-t^2/2} = \sum_{n=0}^{\infty} \frac{(-1)^n (t^2/2)^n}{n!} = \sum_{n=0}^{\infty} \frac{(-1)^n t^{2n}}{2^n n!}
\]
Thus, $\varphi(t) = e^{-t^2/2}$.
\end{example}
\textbf{Remark:} The characteristic function of a random variable \( aX+b \) (where \( a,b \in \mathbb{R} \)) is given by:
\[
\varphi_{aX+b}(t) = \mathbb{E}\left[e^{it(aX+b)}\right] = e^{itb} \mathbb{E}\left[e^{itaX}\right] = e^{itb} \varphi_X(at).
\]
This expression uses the fact that the characteristic function of \( X \) evaluated at \( at \) can be modified by a shift in the variable corresponding to the addition of \( b \).

In particular, if \( a = -1 \) and \( b = 0 \), the characteristic function of \( -X \) is:
\[
\varphi_{-X}(t) = \varphi_X(-t) \quad \text{for all } t \in \mathbb{R}.
\]

\textbf{Next goal:} Our next goal is to show that the characteristic function determines uniquely the law (or the distribution) of a random variable.


\begin{theorem}[Theorem 26.2.]
Two parts of the theorem:
\begin{enumerate}
    \item[(a)] Let \(\mu\) be a probability measure on \(\mathbb{R}\). Let \(\varphi(t)\) be the characteristic function of \(\mu\). If \(a, b \in \mathbb{R}\) are such that \(\mu(\{a\}) = 0\) and \(\mu(\{b\}) = 0\), then
    \[
    \mu((a,b]) = \lim_{T \to \infty} \frac{1}{2\pi} \int_{-T}^{T} \frac{e^{-ita} - e^{-itb}}{it} \varphi(t) \, dt.
    \]
    \textbf{Convention:} In this formula, the function \(\frac{e^{-ita} - e^{-itb}}{it}\) is defined for \(t = 0\) to be equal to \(b-a\) (by l'Hopital's Rule).

    \item[(b)] Let \(\mu\) and \(\nu\) be probability measures on \((\mathbb{R}, \mathcal{B}(\mathbb{R}))\). If \(\mu\) and \(\nu\) have the same characteristic function, then \(\mu = \nu\).
\end{enumerate}
\end{theorem}

